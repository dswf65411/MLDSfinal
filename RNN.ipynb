{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "hw_dir = '/home/yao/gm/mlds/final'\n",
    "# hw_dir = '/Users/Apple/Desktop/MLDS/final'\n",
    "# hw_dir = '/Users/yao/Desktop/trueman/big4/MLDS/final'\n",
    "test_max_slen = 20\n",
    "train_softmax_slen = 50\n",
    "train_max_slen = 70\n",
    "\n",
    "max_slen = 43\n",
    "min_slen = 7\n",
    "\n",
    "batch_size = 1280\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('%s/all_pickles'%hw_dir,'rb') as f1:\n",
    "    [train_data,test_data,word2ix,ix2word,vocab_size] = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101930, 9264)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In[4]:\n",
    "\n",
    "blank_ix = word2ix[' ']\n",
    "new_data = []\n",
    "for data in train_data:\n",
    "    temp = []\n",
    "    sc = 0\n",
    "    if(int(data[0]) < 3):\n",
    "        sc = 0\n",
    "    elif(int(data[0]) > 3):\n",
    "        sc = 1\n",
    "    else:\n",
    "        continue\n",
    "    temp.append(sc)\n",
    "    temp.append([int(x) for x in data[1].split(\" \")])\n",
    "    new_data.append(temp)\n",
    "train_data = new_data\n",
    "train_data = [i for i in train_data if len(i[1])<=70 and len(i[1])>=5]\n",
    "\n",
    "blank_ix = word2ix[' ']\n",
    "new_data = []\n",
    "for data in test_data:\n",
    "    temp = []\n",
    "    sc = 0\n",
    "    if(int(data[0]) < 3):\n",
    "        sc = 0\n",
    "    elif(int(data[0]) > 3):\n",
    "        sc = 1\n",
    "    else:\n",
    "        continue\n",
    "    temp.append(sc)\n",
    "    temp.append([int(x) for x in data[1].split(\" \")])\n",
    "    new_data.append(temp)\n",
    "test_data = new_data\n",
    "\n",
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "train_data_dict = dict()\n",
    "for i in train_data:\n",
    "    len_cmmt = len(i[1])\n",
    "    if len_cmmt in train_data_dict:\n",
    "        train_data_dict[len_cmmt].append(i)\n",
    "    else:\n",
    "        train_data_dict[len_cmmt] = [i]\n",
    "\n",
    "# del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "train_data_dict = dict()\n",
    "for i in test_data:\n",
    "    len_cmmt = len(i[1])\n",
    "    if len_cmmt in train_data_dict:\n",
    "        train_data_dict[len_cmmt].append(i)\n",
    "    else:\n",
    "        train_data_dict[len_cmmt] = [i]\n",
    "\n",
    "# del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amax = [len(i[1]) for i in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(amax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_pos    = max_slen  # 現在在長度多少的file，從長的開始\n",
    "pad_ix = word2ix['<PAD>']\n",
    "batch_pos  = 0\n",
    "def get_batch(batch_size):\n",
    "    global train_data_dict, len_pos, batch_pos\n",
    "    batch_size = (batch_size//len_pos)+1 # 越長的句子一個batch就讀越少句，+1是避免0的狀況\n",
    "    # ====================回傳用的參數=============================\n",
    "    is_eoi = False  # end of iteration\n",
    "    is_eof = False\n",
    "    slen  = len_pos\n",
    "    # ===========================================================\n",
    "    # ====================從檔案中讀batch_size句===================\n",
    "#     try:\n",
    "    now_datas = train_data_dict[len_pos][batch_pos:batch_pos+batch_size]\n",
    "#     except:\n",
    "#         return False,True,None,None,0,0\n",
    "    batch_pos+=batch_size\n",
    "    if len(now_datas)<batch_size:\n",
    "        batch_pos=0\n",
    "        is_eof = True\n",
    "    # ===========================================================\n",
    "    batch_size = len(now_datas) # 因為有可能在檔案的尾巴 讀不滿完整的batch_size 故在此更新batch_size\n",
    "    # ==================若現在的檔案讀完============================\n",
    "    if is_eof == True:\n",
    "        random.shuffle(train_data_dict[len_pos])\n",
    "        if len_pos == min_slen: # 一個iteration跑完 重新再跑\n",
    "            len_pos = max_slen\n",
    "            is_eoi = True\n",
    "        else:                      # 現在這長度的file完了 開始讀len_pos-1的檔案\n",
    "            while len_pos >= min_slen:\n",
    "                len_pos -= 1\n",
    "                if len_pos in train_data_dict:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        if batch_size == 0: # 剛好讀到尾巴了\n",
    "            return is_eoi,is_eof,None,None,slen,batch_size\n",
    "    # ============================================================\n",
    "        \n",
    "    score = np.array([i[0] for i in now_datas])\n",
    "    cmmt = np.array([i[1] for i in now_datas])\n",
    "    return is_eoi,is_eof,score,cmmt,slen,batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "\n",
    "lr = 0.001                   \n",
    "training_iters = 100      \n",
    "\n",
    "n_vocab_size = vocab_size\n",
    "n_filter = 64\n",
    "n_filter_size = 4\n",
    "n_classes = 2\n",
    "n_max_pool = 2\n",
    "n_hidden_units = 512\n",
    "n_embed_size = 300\n",
    "n_batch_size = 5000\n",
    "rnn_layer_num = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "word_embed = tf.Variable(tf.random_uniform([vocab_size,n_embed_size]))\n",
    "y_embed = tf.constant(np.eye(n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # CNN\n",
    "\n",
    "# In[12]:\n",
    "\n",
    "# n_sentence_length = tf.placeholder(tf.int32,[1])\n",
    "x = tf.placeholder(tf.int32, [None, None])\n",
    "# [2,4,3]\n",
    "y = tf.placeholder(tf.int32, [None,])\n",
    "ph_batch_size = tf.placeholder(tf.int32,[])\n",
    "ph_sen_len = tf.placeholder(tf.int32,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sen_index = x\n",
    "x_emb = tf.nn.embedding_lookup(word_embed,sen_index)\n",
    "y_index = y\n",
    "y_emb = tf.nn.embedding_lookup(y_embed,y_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[17]:\n",
    "\n",
    "inW = tf.Variable(tf.random_normal([n_embed_size, n_hidden_units]))\n",
    "outW = tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "inB = tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ]))\n",
    "outB = tf.Variable(tf.constant(0.1, shape=[n_classes, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_steps = ph_sen_len\n",
    "RNN_input1 = tf.reshape(x_emb,[-1,n_embed_size])\n",
    "X_in = tf.matmul(RNN_input1, inW) + inB\n",
    "X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "X_in = tf.nn.relu(X_in)\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0)\n",
    "mul_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]*rnn_layer_num)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(mul_lstm, X_in,dtype=tf.float32)\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])[-1]\n",
    "outputs1 = tf.reshape(outputs,[-1,n_hidden_units])\n",
    "pred = tf.matmul(outputs1, outW) + outB\n",
    "pred_argmax = tf.argmax(pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sin_lstm = tf.contrib.rnn.LSTMCell(lstm_dim, forget_bias=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[19]:\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_emb, logits=pred))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_emb, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# a = sess.run(word_embed)\n",
    "# print(a)\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter:0 batch:1 loss:0.806436 accuracy:0.299145 43\n",
      "iter:0 batch:2 loss:13.531912 accuracy:0.786325 43\n",
      "iter:0 batch:3 loss:23.922825 accuracy:0.290598 43\n",
      "iter:0 batch:4 loss:12.259099 accuracy:0.735043 43\n",
      "iter:0 batch:5 loss:1.903592 accuracy:0.324786 43\n",
      "iter:0 batch:6 loss:5.122417 accuracy:0.803419 43\n",
      "iter:0 batch:7 loss:8.382551 accuracy:0.658120 43\n",
      "iter:0 batch:8 loss:2.192862 accuracy:0.692308 43\n",
      "iter:0 batch:9 loss:11.926012 accuracy:0.264957 43\n",
      "iter:0 batch:10 loss:4.687327 accuracy:0.341880 43\n",
      "iter:0 batch:11 loss:1.678669 accuracy:0.743590 43\n",
      "iter:0 batch:12 loss:3.768550 accuracy:0.684210 43\n",
      "iter:0 batch:13 loss:1.943956 accuracy:0.841667 42\n",
      "iter:0 batch:14 loss:2.361493 accuracy:0.783333 42\n",
      "iter:0 batch:15 loss:2.439878 accuracy:0.700000 42\n",
      "iter:0 batch:16 loss:1.227723 accuracy:0.691667 42\n",
      "iter:0 batch:17 loss:1.519044 accuracy:0.158333 42\n",
      "iter:0 batch:18 loss:1.304616 accuracy:0.241667 42\n",
      "iter:0 batch:19 loss:0.569487 accuracy:0.750000 42\n",
      "iter:0 batch:20 loss:0.934285 accuracy:0.725000 42\n",
      "iter:0 batch:21 loss:1.204166 accuracy:0.700000 42\n",
      "iter:0 batch:22 loss:0.888214 accuracy:0.766667 42\n",
      "iter:0 batch:23 loss:0.662719 accuracy:0.791667 42\n",
      "iter:0 batch:24 loss:0.797720 accuracy:0.650000 42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5ae2ccae36cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mcmmt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mph_batch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mph_sen_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mslen\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         b = sess.run(RNN_input1,inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#         c = sess.run(inW,inp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yao/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yao/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yao/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/yao/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yao/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "for i in range(training_iters):\n",
    "    is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "    batch_now = 0\n",
    "    while(not is_eoi):\n",
    "        batch_now += 1\n",
    "        if batch_size == 0:\n",
    "            is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "            continue\n",
    "        inp = {x:cmmt,y:score,ph_batch_size:batch_size,ph_sen_len:slen}\n",
    "        _,c,a = sess.run([train_op,cost,accuracy],inp)\n",
    "#         b = sess.run(RNN_input1,inp)\n",
    "#         c = sess.run(inW,inp)\n",
    "#         input()\n",
    "        print(\"iter:%d batch:%d loss:%f accuracy:%f %d\"%(i,batch_now,c,a,slen))\n",
    "        # if(batch_now % 10 == 0):\n",
    "        #     loss,acc = sess.run([cost,accuracy],inp)\n",
    "        #     print(\"iter:%d batch:%d loss:%f accuracy:%f\"%(i,batch_now,loss,acc))`\n",
    "        if (batch_now % 100 == 0):\n",
    "            save_path = saver.save(sess,\"./RNN1_model.bin\")\n",
    "        is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 512)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'%s/RNN2_model.bin'%hw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ans_list = []\n",
    "is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "batch_now = 0\n",
    "while(not is_eoi):\n",
    "    batch_now += 1\n",
    "    if batch_size == 0:\n",
    "        is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "        print(\"fuck you\")\n",
    "        continue\n",
    "    inp = {x:cmmt,y:score,ph_batch_size:batch_size,ph_sen_len:slen}\n",
    "    ans = sess.run(pred_argmax,inp)\n",
    "    ans_list.append([cmmt,score,ans])\n",
    "    is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ans_list2 = []\n",
    "for ans in ans_list:\n",
    "    bsize = ans[0].shape[0]\n",
    "    ans[0] = ans[0].tolist()\n",
    "    ans[1] = ans[1].tolist()\n",
    "    ans[2] = ans[2].tolist()\n",
    "    for i in range(bsize):\n",
    "        cmmt = ''.join([ix2word[j] for j in ans[0][i]])\n",
    "        ans_list2.append([cmmt,ans[1][i],ans[2][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yes = 0\n",
    "yes_no = 0\n",
    "for i in ans_list2:\n",
    "    if i[1]==i[2]:\n",
    "        yes+=1\n",
    "    yes_no +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.927537796976242"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes/float(yes_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "different_RNN = [ans for ans in ans_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('%s/output_diff_RNN.bin'%hw_dir,'wb') as f1:\n",
    "    pickle.dump(different_RNN,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
