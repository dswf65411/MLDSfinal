{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import collections\n",
    "import time\n",
    "import pickle\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "hw_dir = '/home/yao/gm/mlds/final'\n",
    "# hw_dir = '/Users/Apple/Desktop/MLDS/final'\n",
    "# hw_dir = '/Users/yao/Desktop/trueman/big4/MLDS/final'\n",
    "test_max_slen = 20\n",
    "train_softmax_slen = 50\n",
    "train_max_slen = 70\n",
    "\n",
    "max_slen = 43\n",
    "min_slen = 7\n",
    "\n",
    "batch_size = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open('%s/all_pickles'%hw_dir,'rb') as f1:\n",
    "    [train_data,test_data,word2ix,ix2word,vocab_size] = pickle.load(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101930, 9264)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blank_ix = word2ix[' ']\n",
    "new_data = []\n",
    "for data in train_data:\n",
    "    temp = []\n",
    "    sc = 0\n",
    "    if(int(data[0]) < 3):\n",
    "        sc = 0\n",
    "    elif(int(data[0]) > 3):\n",
    "        sc = 1\n",
    "    else:\n",
    "        continue\n",
    "    temp.append(sc)\n",
    "    temp.append([int(x) for x in data[1].split(\" \")])\n",
    "    new_data.append(temp)\n",
    "train_data = new_data\n",
    "train_data = [i for i in train_data if len(i[1])<=70 and len(i[1])>=5]\n",
    "\n",
    "blank_ix = word2ix[' ']\n",
    "new_data = []\n",
    "for data in test_data:\n",
    "    temp = []\n",
    "    sc = 0\n",
    "    if(int(data[0]) < 3):\n",
    "        sc = 0\n",
    "    elif(int(data[0]) > 3):\n",
    "        sc = 1\n",
    "    else:\n",
    "        continue\n",
    "    temp.append(sc)\n",
    "    temp.append([int(x) for x in data[1].split(\" \")])\n",
    "    new_data.append(temp)\n",
    "test_data = new_data\n",
    "\n",
    "len(train_data),len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_data_dict = dict()\n",
    "for i in test_data:\n",
    "    len_cmmt = len(i[1])\n",
    "    if len_cmmt in train_data_dict:\n",
    "        train_data_dict[len_cmmt].append(i)\n",
    "    else:\n",
    "        train_data_dict[len_cmmt] = [i]\n",
    "\n",
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "len_pos    = max_slen  # 現在在長度多少的file，從長的開始\n",
    "pad_ix = word2ix['<PAD>']\n",
    "batch_pos  = 0\n",
    "def get_batch(batch_size):\n",
    "    global train_data_dict, len_pos, batch_pos\n",
    "    batch_size = (batch_size//len_pos)+1 # 越長的句子一個batch就讀越少句，+1是避免0的狀況\n",
    "    # ====================回傳用的參數=============================\n",
    "    is_eoi = False  # end of iteration\n",
    "    is_eof = False\n",
    "    slen  = len_pos\n",
    "    # ===========================================================\n",
    "    # ====================從檔案中讀batch_size句===================\n",
    "#     try:\n",
    "    now_datas = train_data_dict[len_pos][batch_pos:batch_pos+batch_size]\n",
    "#     except:\n",
    "#         return False,True,None,None,0,0\n",
    "    batch_pos+=batch_size\n",
    "    if len(now_datas)<batch_size:\n",
    "        batch_pos=0\n",
    "        is_eof = True\n",
    "    # ===========================================================\n",
    "    batch_size = len(now_datas) # 因為有可能在檔案的尾巴 讀不滿完整的batch_size 故在此更新batch_size\n",
    "    # ==================若現在的檔案讀完============================\n",
    "    if is_eof == True:\n",
    "        random.shuffle(train_data_dict[len_pos])\n",
    "        if len_pos == min_slen: # 一個iteration跑完 重新再跑\n",
    "            len_pos = max_slen\n",
    "            is_eoi = True\n",
    "        else:                      # 現在這長度的file完了 開始讀len_pos-1的檔案\n",
    "            while len_pos >= min_slen:\n",
    "                len_pos -= 1\n",
    "                if len_pos in train_data_dict:\n",
    "                    break\n",
    "            \n",
    "            \n",
    "        if batch_size == 0: # 剛好讀到尾巴了\n",
    "            return is_eoi,is_eof,None,None,slen,batch_size\n",
    "    # ============================================================\n",
    "        \n",
    "    score = np.array([i[0] for i in now_datas])\n",
    "    cmmt = np.array([i[1] for i in now_datas])\n",
    "    return is_eoi,is_eof,score,cmmt,slen,batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr = 0.0001                   \n",
    "training_iters = 100      \n",
    "\n",
    "n_vocab_size = vocab_size\n",
    "n_filter = 64\n",
    "n_filter_size = 4\n",
    "n_classes = 2\n",
    "n_max_pool = 2\n",
    "n_hidden_units = 512\n",
    "n_embed_size = 300\n",
    "n_batch_size = 5000\n",
    "rnn_layer_num = 4\n",
    "filter_sizes = np.array([3,4,5])\n",
    "n_filter_kind = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_embed = tf.Variable(tf.random_uniform([vocab_size,n_embed_size]))\n",
    "y_embed = tf.constant(np.eye(n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.int32, [None, None])\n",
    "# [2,4,3]\n",
    "y = tf.placeholder(tf.int32, [None,])\n",
    "ph_batch_size = tf.placeholder(tf.int32,[])\n",
    "ph_sen_len = tf.placeholder(tf.int32,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sen_index = x\n",
    "x_emb = tf.nn.embedding_lookup(word_embed,sen_index)\n",
    "y_index = y\n",
    "y_emb = tf.nn.embedding_lookup(y_embed,y_index)\n",
    "\n",
    "x_4dim = tf.reshape(x_emb,[-1,ph_sen_len,1,n_embed_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "max_pooled_output = []\n",
    "for i, filter_size in enumerate(filter_sizes):\n",
    "    # Convolution layer\n",
    "    filter_shape = [filter_size, 1 , n_embed_size , n_filter]\n",
    "\n",
    "    # initialize weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"weight\")\n",
    "    bias = tf.Variable(tf.constant(0.1, shape=[n_filter]), name=\"bias\")\n",
    "    conv = tf.nn.conv2d(\n",
    "        x_4dim,\n",
    "        weight,\n",
    "        strides=[1,1,1,1],\n",
    "        padding=\"SAME\",    #narrow conv\n",
    "        name=\"conv\"\n",
    "    )\n",
    "    # 過relu\n",
    "    h = tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n",
    "    #[batch,seq_len,1,fil]\n",
    "\n",
    "    # max-pooling\n",
    "    # ksize 這邊的 第二維 就是指filter 總共會滑過去幾次 -> sequence_length - filter_size + 1\n",
    "    # input : [1,0,3,4] , filter_size : 2 , output: [[1,0],[0,3],[3,4]] -> length : 4 - 2 + 1 = 3 \n",
    "    pooled = tf.nn.max_pool(\n",
    "        h,\n",
    "        ksize=[1,n_max_pool, 1, 1],     \n",
    "        strides=[1, n_max_pool, 1, 1],\n",
    "        padding='VALID',\n",
    "        name=\"pool\" \n",
    "    )\n",
    "    #[batch,seq_len/2,1,fil]\n",
    "    max_pooled_output.append(pooled)\n",
    "    #[batch,seq_len/2,1,fil]*3\n",
    "# 合併 所有的 feature\n",
    "num_filters_total = n_filter * len(filter_sizes)\n",
    "h_pool = tf.concat(max_pooled_output, 3)\n",
    "#[batch,seq_len/2,1,fil*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "RNN_input = tf.reshape(h_pool,[ph_batch_size,tf.shape(h_pool)[1],-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inW = tf.Variable(tf.random_normal([n_filter*n_filter_kind, n_hidden_units]))\n",
    "outW = tf.Variable(tf.random_normal([n_hidden_units, n_classes]))\n",
    "inB = tf.Variable(tf.constant(0.1, shape=[n_hidden_units, ]))\n",
    "outB = tf.Variable(tf.constant(0.1, shape=[n_classes, ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_steps = tf.shape(RNN_input)[1]\n",
    "RNN_input1 = tf.reshape(RNN_input,[-1,n_filter*n_filter_kind])#[batch*seq_len/2,fil*3]\n",
    "X_in = tf.matmul(RNN_input1, inW) + inB#[batch*seq_len/2,hidden]\n",
    "X_in = tf.reshape(X_in, [-1, n_steps, n_hidden_units])\n",
    "##[batch,seq_len/2,hidden]\n",
    "X_in = tf.nn.relu(X_in)\n",
    "lstm_cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_units, forget_bias=1.0)\n",
    "mul_lstm = tf.contrib.rnn.MultiRNNCell([lstm_cell]*rnn_layer_num)\n",
    "outputs, final_state = tf.nn.dynamic_rnn(mul_lstm, X_in,dtype=tf.float32)\n",
    "##[batch,seq_len/2,hidden]\n",
    "outputs = tf.transpose(outputs, [1, 0, 2])[-1]\n",
    "#[1,batch,hidden]\n",
    "outputs1 = tf.reshape(outputs,[-1,n_hidden_units])\n",
    "#[batch,hidden]\n",
    "pred = tf.matmul(outputs1, outW) + outB\n",
    "#[batch,n_class]\n",
    "pred_argmax = tf.argmax(pred, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_emb, logits=pred))\n",
    "train_op = tf.train.AdamOptimizer(lr).minimize(cost)\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y_emb, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "for i in range(training_iters):\n",
    "    is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "    batch_now = 0\n",
    "    while(not is_eoi):\n",
    "        batch_now += 1\n",
    "        if batch_size == 0:\n",
    "            is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "            continue\n",
    "        inp = {x:cmmt,y:score,ph_batch_size:batch_size,ph_sen_len:slen}\n",
    "        _,c,a = sess.run([train_op,cost,accuracy],inp)\n",
    "        print(\"iter:%d batch:%d loss:%f accuracy:%f %d\"%(i,batch_now,c,a,slen))\n",
    "        # if(batch_now % 10 == 0):\n",
    "        #     loss,acc = sess.run([cost,accuracy],inp)\n",
    "        #     print(\"iter:%d batch:%d loss:%f accuracy:%f\"%(i,batch_now,loss,acc))`\n",
    "        # if (batch_now % 100 == 0):\n",
    "        #     save_path = saver.save(sess,\"./RCNN3_model.bin\")\n",
    "        is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'%s/RCNN3_model.bin'%hw_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ans_list = []\n",
    "is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "batch_now = 0\n",
    "while(not is_eoi):\n",
    "    batch_now += 1\n",
    "    if batch_size == 0:\n",
    "        is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)\n",
    "        print(\"fuck you\")\n",
    "        continue\n",
    "    inp = {x:cmmt,y:score,ph_batch_size:batch_size,ph_sen_len:slen}\n",
    "    ans = sess.run(pred_argmax,inp)\n",
    "    ans_list.append([cmmt,score,ans])\n",
    "    is_eoi,is_eof,score,cmmt,slen,batch_size = get_batch(n_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ans_list2 = []\n",
    "for ans in ans_list:\n",
    "    bsize = ans[0].shape[0]\n",
    "    ans[0] = ans[0].tolist()\n",
    "    ans[1] = ans[1].tolist()\n",
    "    ans[2] = ans[2].tolist()\n",
    "    for i in range(bsize):\n",
    "        cmmt = ''.join([ix2word[j] for j in ans[0][i]])\n",
    "        ans_list2.append([cmmt,ans[1][i],ans[2][i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "yes = 0\n",
    "yes_no = 0\n",
    "for i in ans_list2:\n",
    "    if i[1]==i[2]:\n",
    "        yes+=1\n",
    "    yes_no +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9315334773218142"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes/float(yes_no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9260"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ans_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "different = [ans for ans in ans_list2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9260"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(different)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "different_RCNN = different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('%s/output_diff_RCNN.bin'%hw_dir,'wb') as f1:\n",
    "    pickle.dump(different_RCNN,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/yao/gm/mlds/final'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
